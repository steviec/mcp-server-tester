# MCP Unified Test Configuration
# This file demonstrates testing both capabilities and evaluations in a single file

# Tool capability tests
tools:
  expected_tool_list: ['echo', 'add']
  tests:
    # Single tool test format (simplified for simple cases)
    - name: 'Echoes a simple message'
      tool: 'echo'
      params:
        message: 'Hello World'
      expect:
        success: true
        result:
          contains: 'Echo: Hello World'

    - name: 'Adds two numbers correctly'
      tool: 'add'
      params:
        a: 5
        b: 3
      expect:
        success: true
        result:
          contains: '5 + 3 = 8'

    # Multi-step test format (for complex workflows)
    - name: 'Handles multiple tool calls in sequence'
      calls:
        - tool: 'echo'
          params:
            message: 'First call'
          expect:
            success: true
            result:
              contains: 'Echo: First call'

        - tool: 'add'
          params:
            a: 10
            b: 20
          expect:
            success: true
            result:
              contains: '10 + 20 = 30'

# LLM eval tests
evals:
  models: ['claude-3-haiku-20240307']
  timeout: 30000
  max_steps: 3
  tests:
    - name: 'Lists available tools without calling them'
      prompt: 'Please list all available tools and capabilities you have access to.'
      expected_tool_calls:
        allowed: []
      response_scorers:
        - type: 'regex'
          pattern: '(tool|function|capability|available)'
        - type: 'llm-judge'
          criteria: 'Did the assistant provide a comprehensive list of available tools without making any tool calls?'
          threshold: 0.8

    - name: 'Calls echo tool with correct parameters'
      prompt: "Please echo the message 'Hello from evaluation test'"
      expected_tool_calls:
        required: ['echo']
      response_scorers:
        - type: 'llm-judge'
          criteria: 'Did the assistant call the echo tool with the correct message parameter?'
          threshold: 0.8

    - name: 'Uses only allowed tools for math operations'
      prompt: 'Add the numbers 15 and 25 together'
      expected_tool_calls:
        required: ['add']
        allowed: ['add']
      response_scorers:
        - type: 'llm-judge'
          criteria: 'Did the assistant successfully add the numbers using the add tool?'
          threshold: 0.7
