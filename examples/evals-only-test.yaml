# MCP Evaluations-Only Test Configuration
# This file demonstrates testing only LLM evaluations

evaluations:
  models: ['claude-3-haiku-20240307']
  timeout: 30000
  max_steps: 3
  tests:
    - name: 'Lists available tools without calling them'
      prompt: 'Please list all available tools and capabilities you have access to.'
      expected_tool_calls:
        allowed: []
      response_scorers:
        - type: 'regex'
          pattern: '(tool|function|capability|available)'

    - name: 'Calls echo tool with correct parameters'
      prompt: "Please echo the message 'Hello from evaluation test'"
      expected_tool_calls:
        required: ['echo']
      response_scorers:
        - type: 'llm-judge'
          criteria: 'Did the assistant call the echo tool with the correct message parameter?'
          threshold: 0.8
